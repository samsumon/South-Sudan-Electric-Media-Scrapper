{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b4ba34-dd42-44f8-aa8d-8851b9c9cc5e",
   "metadata": {},
   "source": [
    "<h1> Scrape news from sudatribune.com</h1>\n",
    "<h2> Import packages </h2> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07320d88-9c9a-4cc5-9961-766625392e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import csv\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a2b1d4-9ae8-4363-9f22-9b82ce34a5be",
   "metadata": {},
   "source": [
    "# Instructions: \n",
    "This web scraper is capable of extracting information from the articles located on the Sudan Tribune. It will scrape the article date, link, and locations. Each web page is separated by values of 10 in its html code. Understanding this is not important, however understanding the procedure is in order to avoid errors. Located below is a line that reads 'While z < \"...\" ' this must be a multiple of 10. So , for example, if you want to crawl 5 pages of articles, change the statement to read 'While z < 50 ' This will make the scraper go from page 1 to 5. After the program has been ran, simply open up your documents and locate the csv file titled South_Sudan_v2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b723485f-fea4-4fae-b07c-bc33be771358",
   "metadata": {},
   "source": [
    "# Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceabce5a-2d82-4e18-a216-7d35458c0f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "z = 0\n",
    "dates = []\n",
    "articles = []\n",
    "locations = []\n",
    "articles_words = []\n",
    "links = []\n",
    "link_words = []\n",
    "url = 'https://www.sudantribune.com/spip.php?rubrique1#pagination_articles'\n",
    "while z < 20:\n",
    "    source = requests.get(url)\n",
    "    soup = BeautifulSoup(source.content, 'html.parser')\n",
    "    print('You have just crawled the website:' + url + '!')\n",
    "    print('\\n')\n",
    "    # Article Dates\n",
    "    for date in soup.find_all('li'):\n",
    "        small = date.find(\"small\")\n",
    "        if small is None:\n",
    "            print(\"\", end=\"\")\n",
    "        else:\n",
    "            dates.append(small.get_text())\n",
    "    # Links\n",
    "    ss_h3 = soup.find_all('h3')\n",
    "    for ss in ss_h3:\n",
    "        links.append(ss.find('a')['href'])\n",
    "    # Article introductions\n",
    "    for story in soup.find_all('li'):\n",
    "        span = story.find('span')\n",
    "        if span is None:\n",
    "            print(\"\", end=\"\")\n",
    "        else:\n",
    "            articles.append(span.get_text())\n",
    "    x = 0\n",
    "    while x < len(articles):\n",
    "        articles_words.append(re.findall(r'\\w+', articles[x]))\n",
    "        x = x + 1\n",
    "    # Geographical locations\n",
    "    y = 0\n",
    "    while y < len(articles_words):\n",
    "        locations.append(articles_words[y][3])\n",
    "        y = y + 1\n",
    "    z = z + 10\n",
    "    url = 'https://www.sudantribune.com/spip.php?rubrique1&debut_articles=' + str(z) + '#pagination_articles'\n",
    "with open(\"South_Sudan_v2.csv\", \"w\", newline=\"\") as new_file:\n",
    "    csv_writer = csv.writer(new_file, delimiter=',')\n",
    "    csv_writer.writerow(['Date', 'Location', 'Links'])\n",
    "    for i in range(0, 10):\n",
    "        csv_writer.writerows([[dates[i], locations[i], \"https://www.sudantribune.com\" + links[i]]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bea9aee-90fb-464b-aeb1-a56f3c38f1c7",
   "metadata": {},
   "source": [
    "Special thanks to Stephen McIltrot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a6fe3a-34c4-47ab-8435-f03475b80bff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
